{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import HorseZebraDataset\n",
    "import sys\n",
    "from utils import save_checkpoint, load_checkpoint\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import config\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "from discriminator_model import Discriminator\n",
    "from generator_model import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler):\n",
    "    H_reals = 0\n",
    "    H_fakes = 0\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for idx, (zebra, horse) in enumerate(loop):\n",
    "        zebra = zebra.to(config.DEVICE)\n",
    "        horse = horse.to(config.DEVICE)\n",
    "\n",
    "        # Train Discriminators H and Z\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake_horse = gen_H(zebra)\n",
    "            D_H_real = disc_H(horse)\n",
    "            D_H_fake = disc_H(fake_horse.detach())\n",
    "            H_reals += D_H_real.mean().item()\n",
    "            H_fakes += D_H_fake.mean().item()\n",
    "            D_H_real_loss = mse(D_H_real, torch.ones_like(D_H_real))\n",
    "            D_H_fake_loss = mse(D_H_fake, torch.zeros_like(D_H_fake))\n",
    "            D_H_loss = D_H_real_loss + D_H_fake_loss\n",
    "\n",
    "            fake_zebra = gen_Z(horse)\n",
    "            D_Z_real = disc_Z(zebra)\n",
    "            D_Z_fake = disc_Z(fake_zebra.detach())\n",
    "            D_Z_real_loss = mse(D_Z_real, torch.ones_like(D_Z_real))\n",
    "            D_Z_fake_loss = mse(D_Z_fake, torch.zeros_like(D_Z_fake))\n",
    "            D_Z_loss = D_Z_real_loss + D_Z_fake_loss\n",
    "\n",
    "            # put it togethor\n",
    "            D_loss = (D_H_loss + D_Z_loss)/2\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        d_scaler.scale(D_loss).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        # Train Generators H and Z\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # adversarial loss for both generators\n",
    "            D_H_fake = disc_H(fake_horse)\n",
    "            D_Z_fake = disc_Z(fake_zebra)\n",
    "            loss_G_H = mse(D_H_fake, torch.ones_like(D_H_fake))\n",
    "            loss_G_Z = mse(D_Z_fake, torch.ones_like(D_Z_fake))\n",
    "\n",
    "            # cycle loss\n",
    "            cycle_zebra = gen_Z(fake_horse)\n",
    "            cycle_horse = gen_H(fake_zebra)\n",
    "            cycle_zebra_loss = l1(zebra, cycle_zebra)\n",
    "            cycle_horse_loss = l1(horse, cycle_horse)\n",
    "\n",
    "            # identity loss (remove these for efficiency if you set lambda_identity=0)\n",
    "            identity_zebra = gen_Z(zebra)\n",
    "            identity_horse = gen_H(horse)\n",
    "            identity_zebra_loss = l1(zebra, identity_zebra)\n",
    "            identity_horse_loss = l1(horse, identity_horse)\n",
    "\n",
    "            # add all togethor\n",
    "            G_loss = (\n",
    "                loss_G_Z\n",
    "                + loss_G_H\n",
    "                + cycle_zebra_loss * config.LAMBDA_CYCLE\n",
    "                + cycle_horse_loss * config.LAMBDA_CYCLE\n",
    "                + identity_horse_loss * config.LAMBDA_IDENTITY\n",
    "                + identity_zebra_loss * config.LAMBDA_IDENTITY\n",
    "            )\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(G_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "\n",
    "        if idx % 200 == 0:\n",
    "            save_image(fake_horse*0.5+0.5, f\"saved_images/horse_{idx}.png\")\n",
    "            save_image(fake_zebra*0.5+0.5, f\"saved_images/zebra_{idx}.png\")\n",
    "\n",
    "        loop.set_postfix(H_real=H_reals/(idx+1), H_fake=H_fakes/(idx+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadrien/.local/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/1334 [00:00<?, ?it/s]/home/hadrien/.local/lib/python3.7/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "disc_H = Discriminator(in_channels=3).to(config.DEVICE)\n",
    "disc_Z = Discriminator(in_channels=3).to(config.DEVICE)\n",
    "gen_Z = Generator(img_channels=3, num_residuals=9).to(config.DEVICE)\n",
    "gen_H = Generator(img_channels=3, num_residuals=9).to(config.DEVICE)\n",
    "opt_disc = optim.Adam(\n",
    "    list(disc_H.parameters()) + list(disc_Z.parameters()),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "\n",
    "opt_gen = optim.Adam(\n",
    "    list(gen_Z.parameters()) + list(gen_H.parameters()),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "\n",
    "L1 = nn.L1Loss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "if config.LOAD_MODEL:\n",
    "    load_checkpoint(\n",
    "        config.CHECKPOINT_GEN_H, gen_H, opt_gen, config.LEARNING_RATE,\n",
    "    )\n",
    "    load_checkpoint(\n",
    "        config.CHECKPOINT_GEN_Z, gen_Z, opt_gen, config.LEARNING_RATE,\n",
    "    )\n",
    "    load_checkpoint(\n",
    "        config.CHECKPOINT_CRITIC_H, disc_H, opt_disc, config.LEARNING_RATE,\n",
    "    )\n",
    "    load_checkpoint(\n",
    "        config.CHECKPOINT_CRITIC_Z, disc_Z, opt_disc, config.LEARNING_RATE,\n",
    "    )\n",
    "\n",
    "dataset = HorseZebraDataset(\n",
    "    root_horse=config.TRAIN_DIR+\"/horses\", root_zebra=config.TRAIN_DIR+\"/zebras\", transform=config.transforms\n",
    ")\n",
    "val_dataset = HorseZebraDataset(\n",
    "    root_horse=config.VAL_DIR + \"/horses\", root_zebra=config.VAL_DIR +\"/zebras\", transform=config.transforms\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "g_scaler = torch.cuda.amp.GradScaler()\n",
    "d_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    train_fn(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler)\n",
    "\n",
    "    if config.SAVE_MODEL:\n",
    "        save_checkpoint(gen_H, opt_gen, filename=config.CHECKPOINT_GEN_H)\n",
    "        save_checkpoint(gen_Z, opt_gen, filename=config.CHECKPOINT_GEN_Z)\n",
    "        save_checkpoint(disc_H, opt_disc, filename=config.CHECKPOINT_CRITIC_H)\n",
    "        save_checkpoint(disc_Z, opt_disc, filename=config.CHECKPOINT_CRITIC_Z)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
